# SparkDmo
Spark的实验
* [项目一级数的前n项之和](#项目一级数的前n项之和)
* [项目二RDD编程](#项目二RDD编程)
* [项目三音乐播放器数据分析](#项目三音乐播放器数据分析)
* [项目四黑名单过滤](#项目四黑名单过滤)
## 开发与运行环境
- CentOS7虚拟机
- Hadoop2.7.1
- Spark2.1.0
- Scala2.11.12
## Spark的特点
- 快速
Spark的中间数据存放于内存中，有更高的迭代运算效率，而Hadoop每次迭代的中间数据存放于HDFS中，涉及硬盘的读写，明显降低了运算效率。
- 易用
Spark支持使用Scala、Python、Java及R语言快速编写应用。同时Spark提供超过80个高级运算符，使得编写并行应用程序变得容易，并且可以在Scala、Python或R的交互模式下使用Spark
- 通用
Spark可以与SQL、Streaming及复杂的分析良好结合。Spark还有一系列的高级工具，包括SparkSQL、MLib（机器学习库）、Graphx（图计算）和SparkStreaming，并且支持在一个应用中同时使用这些组件。
- 随处运行
用户可以使用Spark的独立集群模式运行Spark，也可以在EC2（亚马逊弹性计算云入HadoopYARN或者ApacheMesos上运行Spark，并且可以HDFS、Cassandra、Hbase、Hive、Tachyon和任何分布式文件系统读取数据。
- 代码简洁
Spark支持使用Scala、Python等语言编写。ScalaI或者Python的代码相对Java来说都比较简洁，因此Spark使用scala或者Python编写应用程序要比使用Mapreduce编写应用程序简单方便。比如，Mapreduce实现单词计数可能需要60多行代码，而Spark用Scala语言实现只需要一行。
快：比MR快100倍

易用：支持java/python/scala/R/SQL开发，很多提供了好用的算子

通用：有sparkcore/sparlsql/sparkStreaming/sparkmllib/sparkgrafx

运行范围广：可以运行在yarn/standalone/mesos/k8s上

## Spark主要功能模块

Spark主要有SparkCore、SparkSQL、SparkStreaming、GraphX和MLlib模块组成，主要功能如下：

SparkCore：是集群系统中最核心的功能，包括创建SparkContext对象、任务提交与执行、分布式部署任务与资源、分布式计算等。

SparkSQL：提供了对关系型数据库的增、删、查、改等的交互式操作，也可以对Hive和Json等数据格式的数据进行符合要求的处理。

SparkStreaming：主要功能是将流数据集聚为弹性分布式数据集RDD，然后再进行批量处理，提供大数据流式计算处理服务，其数据吞吐量较大。支持的数据源包括Kafka、Twitter、MQTT、Flume、ZeroMQ和简单的TCP套接字等。

GraphX：主要功能是通过调用其中的API，解决基于分布式的内存图形计算问题。在迭代计算次数较多的情况下，图形计算和处理效率较高。

MLlib：通过调用其中的API接口，实现大量的机器学习有关的分类、统计、回归等多种功能。方便了用户，提高了效率，同时也大大降低了用户的学习成本。
## 项目一级数的前n项之和
### 内容
请用脚本的方式编程计算并输出下列级数的前n项之和Sn.直到Sn刚好大于或等于为止,其中q为大于的整数.其值通过键盘输入。
Sn=2/1+3/2+4/3+…+(n+1)/n

例如，若q的值为50.0,则输出应为:Sn=50.416695。请将源文件保存为exercisel.scala,在REPL模式下,使用:loadexercisel.scala命令测试运行,测试样例:q=1,30,50,学号后两位对应的整数(q=1时，Sn=2;q=30时,Sn=30.891459;q=50时,Sn=50.416695)

### 项目设计思路

采用从控制台读入数据，其方法为readDoble()，并导入对象scala.io.Stdln._或者直接用全称进行调用，定义一个变量存放出入的数据，定义一个求和累加变量，便于条件判断。首先要对其输入值进行判断，若输入值大于0，则进行下一步求和，若Sn刚好大于或等于为止,其中q为大于的整数，则输出值；若输入值不大于0，则打印出“error!”。

## 项目二RDD编程
### 内容	
任务一：课程成绩数据处理
在给定的实验数据scores.txt中,参照已有数据,增加本人的姓名和各科成绩(模拟值);
在spark-shell中通过编程来计算以下内容:
(1)该系总共有多少学生;

(2)该系共开设了多少门课程;

(3)Tom同学的平均分是多少;

(4)学生本人的平均分是多少;

(5)该系DataBase课程共有多少人选修;

(6)求每名同学的选修的课程门数;

(7)各门课程的平均分是多少;

任务二：

数据文件：jc_content_viewlog.txt
对数据进行探索与分析：

（1）过滤实训中访问次数在50次以上的用户记录并持久化到内存

（2）统计访问50次以上的用户主要访问的前五类网页

### 项目设计思路
任务一：
先从linux本地文件创建RDD，本地文件读取是通过sc.textFile(“路径”)，在路径前面加上“file://”表示从本地Linux文件系统读取。
（1）现将成绩表的RDD进行数据转换，每条数据的第一个数据分割成一列，表示学生姓名，对其进行去重计数；

（2）现将成绩表的RDD进行数据转换，每条数据的第二个数据分割成一列，表示课程，对其进行去重计数；

（3）现将成绩表的RDD进行数据转换，每条数据的分割成一列，表示学生姓名，课程，成绩，分隔符为“，”，存储为三元组格式，成绩转化为Int类型，可直接通过toInt来转化。通过filter过滤出姓名为“Tom”的学生数据，并通过map操作提取学生成绩，采用mean方法求均值。

（4）（5）都与（3）同理

（6）现将成绩表的RDD进行数据转换，每条数据的分割成一列，表示学生姓名，课程，成绩，分隔符为“，”，存储为三元组格式，成绩转化为Int类型，可直接通过toInt来转化。通过groupBy()对相同键的值进行分组，并计算第二列，即还学生选修的课程。

（7）现将成绩表的RDD进行数据转换，每条数据的分割成一列，表示学生姓名，课程，成绩，分隔符为“，”，存储为键值对格式，成绩转化为Int类型，可直接通过toInt来转化。通过mapValues()，其功能实对键值对每个value都应用一个函数，但是key不会发生变化，即通过键值课程，每个人的该课程的成绩进行累加求和保留对应数据，通过reduceByKey把成绩聚合成一个List，然后对这个List进行排序，再使用MapValues数据计算。

任务二：

（1）读文件，获取所有记录

（2）找到访问网页总次数50次以上的用户（清单）

（3）从第一步的所有记录中过滤出第二步用户的记录其操作与任务一，同理。

## 项目三音乐播放器数据分析
### 内容

文件user_artistdata.txt记录的是用户播放某首音乐的次数,数据包含3个字段,分别为:用户ID,音乐ID和播放次数。
使用SparkSQL相关知识对数据进行分析,分析目标如下:

（1）统计非重复的用户个数。

（2）统计用户听过的歌曲总数。

（3）找出ID为"1000002"的用户最喜欢的10首歌曲(即播放次数最多的10首歌曲。
### 项目设计思路

RDD数据转换为DataFrame有两种方式。此项目是利用反射机制推断RDD模式。先定义caseclass，三个字段的类型都为Int类型，因为只有caseclass才
能被spark隐式地转换为DataFrame。使用SparkContext读取HDFS上的user_artistdata.txt文件，得到一个RDD数据集。

（1）将DataFrame注册成为临时表，然后通过SQL语句进行查询。利用SQL语句进行筛选去重，通过sqlContext.sql()的方法进行查询。

（2）通过用户ID分组，去重并统计数量。

（3）找出ID为“1000002”的用户喜欢的10首歌曲（即播放次数多的10首歌曲，其sql语句为“selectartistid,countfromuser_artistwhereuserid=1000002orderbycountdesclimit10"”

## 项目四黑名单过滤
### 内容
广告计费系统，是点商必不可少的一个功能。为了防止恶意的广告点击(假设商户A和B同时在某电商做了广告，A和B为竞争对手，那么如果A使用点击机器人进行对B的广告的恶意点击，那么B的广告费将很快被用完)，必须对广告点击进行黑名单过滤黑名单的过滤。黑名单的过滤可以是ID，可以是IP等，黑名单就是过滤的条件，利用SparkStreaming的流处理特性，可实现实时黑名单的过滤实现。
模拟SparkStreaming实时过滤黑名单。
	
黑名单数据，第二个字段为true的名单为黑名单。在一个终端窗口中启动监听888端口。

编写SparkStreaming程序，使用leftOuterJoin操作和fiter方法
过滤掉黑名单的数据。

在localhost上启动监听8888端口，输人表2.2所示的内容。编写SparkStreaming程序，使用lefOuterJoin操作及filter方法过滤掉黑名单的数据。

### 项目设计思路

（1）创建StreamingContext对象

（2）创建InputDstream

（3）操作Dstream

（4）启动SparkStreaming：之前的所有不走只是创建了执行流程，程序没有真正连接上数据源，也没有对数据进行任何操作，只是设定好了所有的执行计划，当scc.start()，启动后程序才真正进行所有的预期的操作。

（5）在另一台终端上，输入命令“nc–l8888”，此时可输入监听端口输入表格的内容，验证是否成功
